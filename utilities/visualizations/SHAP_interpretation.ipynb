{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sksurv.metrics          import concordance_index_censored, concordance_index_ipcw\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from matplotlib.colors       import LinearSegmentedColormap\n",
    "from skimage.transform       import resize\n",
    "from plottify                import autosize\n",
    "from sklearn                 import metrics\n",
    "from PIL                     import Image\n",
    "from adjustText              import adjust_text\n",
    "from scipy.cluster           import hierarchy\n",
    "from lifelines \t\t\t\t import CoxPHFitter\n",
    "import statsmodels.api   as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy             as np\n",
    "import seaborn           as sns\n",
    "import pandas            as pd\n",
    "import scanpy            as sc\n",
    "import scipy.stats\n",
    "import matplotlib\n",
    "import anndata\n",
    "import random\n",
    "import fastcluster\n",
    "import sklearn\n",
    "import copy\n",
    "import umap\n",
    "import h5py\n",
    "import shap\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Workspace path.\n",
    "main_path = '/media/adalberto/Disk2/PhD_Workspace'\n",
    "\n",
    "sys.path.append(main_path)\n",
    "from models.visualization.survival import save_fold_KMs\n",
    "from models.visualization.forest_plots import report_forest_plot_cph, summary_cox_forest_plots\n",
    "from models.clustering.cox_proportional_hazard_regression_leiden_clusters import *\n",
    "from models.evaluation.folds import load_existing_split\n",
    "from models.visualization.attention_maps import *\n",
    "from models.clustering.data_processing import *\n",
    "from data_manipulation.data import Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Plot figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_performance_cindex(all_data, title='Leiden + Logistic Regression'):\n",
    "\tmeanprops={\"marker\":\"o\", \"markerfacecolor\":\"red\", \"markeredgecolor\":\"black\", \"markersize\":\"6\"}\n",
    "\tsns.set_theme(style='white')\n",
    "\tylim = (0.4, 1.0)\n",
    "\tfig   = plt.figure(figsize=(10,5))\n",
    "\tax    = fig.add_subplot(1, 1, 1)\n",
    "\tsns.pointplot(x='Leiden', hue='Set', y='C-Index', data=all_data, ax=ax, linewidth=0.2, dodge=.4, join=False, capsize=.02, markers='o')\n",
    "\tif ylim is not None:\n",
    "\t\tax.set_ylim(ylim)\n",
    "\tax.set_title(title, fontweight='bold', fontsize=18)\n",
    "\tax.legend(loc='upper left')\n",
    "\tax.axhline(y=0.75)\n",
    "\tax.axhline(y=0.6)\n",
    "\tplt.show()\n",
    "\n",
    "# Plot SHAP figure.\n",
    "def plot_shap_figure(shap_cv, order, total_order, text1, text2, plot_size=(25,20), fontsize_labels=24, fontsize_ticks=22, offset=150, cmap_bar=sns.diverging_palette(250, 20, as_cmap=True)):\n",
    "\ttext = text1 + ''.join([' ']*(offset-len(text1+text2))) + text2\n",
    "\n",
    "\tsns.set_theme(style='white')\n",
    "\tshap.plots.beeswarm(shap_cv, max_display=len(order)+1, order=total_order,\n",
    "\t\t\t\t\t\tcolor_bar_label='Histomorpholical Phenotype Cluster (HPC)\\n Contribution',\n",
    "\t\t\t\t\t\tcolor=cmap_bar, plot_size=plot_size, show=False)\n",
    "\tfigure = plt.gcf()\n",
    "\tfigure.axes[0].set_xlabel(text, fontweight='bold', fontsize=fontsize_labels)\n",
    "\tfigure.axes[1].set_ylabel('Histomorpholical Phenotype Cluster (HPC)\\n Contribution', fontweight='bold', fontsize=fontsize_labels)\n",
    "\tfor ax in [figure.axes[0], figure.axes[1]]:\n",
    "\t\tfor tick in ax.xaxis.get_major_ticks():\n",
    "\t\t\ttick.label1.set_fontweight('bold')\n",
    "\t\t\ttick.label2.set_fontweight('bold')\n",
    "\t\t\ttick.label1.set_fontsize(fontsize_ticks)\n",
    "\t\t\ttick.label2.set_fontsize(fontsize_ticks)\n",
    "\t\tfor tick in ax.yaxis.get_major_ticks():\n",
    "\t\t\ttick.label1.set_fontweight('bold')\n",
    "\t\t\ttick.label2.set_fontweight('bold')\n",
    "\t\t\ttick.label1.set_fontsize(fontsize_ticks)\n",
    "\t\t\ttick.label2.set_fontsize(fontsize_ticks)\n",
    "\t\tfor axis in ['top','bottom','left','right']:\n",
    "\t\t\tax.spines[axis].set_linewidth(4)\n",
    "\tplt.show()\n",
    "\n",
    "def plot_shap_decision(shap_base_values, shap_values, dataset_samples_features, low_risk_ind, high_risk_ind, num_features, show_legend, text, fontsize_labels, fontsize_ticks, fontsize_legend, l_box_w, xlim=None):\n",
    "\n",
    "    if low_risk_ind is None:\n",
    "        risk_ind = high_risk_ind\n",
    "    elif high_risk_ind is None:\n",
    "        risk_ind = low_risk_ind\n",
    "    else:\n",
    "        risk_ind = np.concatenate([low_risk_ind.values, high_risk_ind.values])\n",
    "\n",
    "    # Legend handling.\n",
    "    if show_legend:\n",
    "        legend_labels = ['Low Risk' for s in range(low_risk_ind.values.shape[0])]\n",
    "        legend_labels.extend(['High Risk' for s in range(high_risk_ind.values.shape[0])])\n",
    "    else:\n",
    "        legend_labels = None\n",
    "    # Number of HPCs to display\n",
    "    if num_features == 'all':\n",
    "        feature_display_range = slice(None, None, -1)\n",
    "    else:\n",
    "        feature_display_range = slice(-1, -num_features, -1)\n",
    "\n",
    "    shap_base_plot = [shap_base_values[risk_ind[0]]]\n",
    "    shap_cv_values = np.array(shap_cv.values[risk_ind[0]].reshape((1,-1)))\n",
    "    data_values    = np.array(dataset_samples_features.loc[risk_ind[0]].values.reshape((1,-1)))\n",
    "\n",
    "    for ind in risk_ind[1:]:        \n",
    "        shap_base_plot.append(shap_base_values[ind])\n",
    "        shap_cv_values = np.concatenate([shap_cv_values, shap_cv.values[ind].reshape((1,-1))])\n",
    "        data_values    = np.concatenate([data_values, dataset_samples_features.loc[ind].values.reshape((1,-1))])\n",
    "\n",
    "    # SHAP decision plot.\n",
    "    shap.decision_plot(base_value=np.mean(shap_base_plot), shap_values=shap_cv_values, features=dataset_samples_features.loc[risk_ind], xlim=xlim,\n",
    "                    feature_order='importance', feature_display_range=feature_display_range, show=False, legend_labels=legend_labels, legend_location='lower right')\n",
    "\n",
    "    # Ascetic fixes.\n",
    "    figure = plt.gcf()\n",
    "    figure.axes[0].set_xlabel(text, fontweight='bold', fontsize=fontsize_labels)\n",
    "    figure.axes[0].set_ylabel('Histomorpholical Phenotype Cluster (HPC)', fontweight='bold', fontsize=fontsize_labels)\n",
    "    for ax in [figure.axes[0]]:\n",
    "        for tick in ax.xaxis.get_major_ticks():\n",
    "            tick.label1.set_fontweight('bold')\n",
    "            tick.label2.set_fontweight('bold')\n",
    "            tick.label1.set_fontsize(fontsize_ticks)\n",
    "            tick.label2.set_fontsize(fontsize_ticks)\n",
    "        for tick in ax.yaxis.get_major_ticks():\n",
    "            tick.label1.set_fontweight('bold')\n",
    "            tick.label2.set_fontweight('bold')\n",
    "            tick.label1.set_fontsize(fontsize_ticks)\n",
    "            tick.label2.set_fontsize(fontsize_ticks)\n",
    "        for axis in ['bottom','left','right']:\n",
    "            ax.spines[axis].set_linewidth(4)\n",
    "    \n",
    "    # Legend for High/Low risk group.\n",
    "    if show_legend:\n",
    "        legend = figure.axes[0].legend_\n",
    "        new_handles = [legend.legendHandles[0], legend.legendHandles[-1]]\n",
    "        new_texts   = [legend.get_texts()[0]._text, legend.get_texts()[-1]._text]\n",
    "        map_dict = dict()\n",
    "        for handle, text in zip(legend.legendHandles, legend.get_texts()):\n",
    "            map_dict[handle._label] = text._text\n",
    "\n",
    "        legend.remove()\n",
    "        legend = figure.axes[0].legend(new_handles, new_texts, loc='lower right')\n",
    "        legend.get_frame().set_linewidth(l_box_w)\n",
    "        for i, handler in enumerate(legend.legendHandles):\n",
    "            handler.set_linewidth(5)\n",
    "            handler.set_color('Blue')\n",
    "            if i!=0:\n",
    "                handler.set_color('Red')\n",
    "\n",
    "        for text in legend.get_texts():\n",
    "            text.set_size(fontsize_legend)\n",
    "            text.set_fontweight('bold')\n",
    "\n",
    "        # Re-color lines for High/Low risk group.\n",
    "        continuous_lines = [child for child in figure.axes[0]._children if child._linestyle == '-' and len(child._x) > 2]\n",
    "        i = 0 \n",
    "        for child in continuous_lines:\n",
    "            risk_name = map_dict[child._label]\n",
    "            if risk_name == 'Low Risk':\n",
    "                child.set_color('Blue')\n",
    "            elif risk_name == 'High Risk':\n",
    "                child.set_color('Red')\n",
    "            i+=1\n",
    "    else:\n",
    "        annotations_val  = [child for child in figure.axes[0]._children if not isinstance(child, matplotlib.lines.Line2D)]\n",
    "        for child in annotations_val:\n",
    "            orig_fontsize = child._fontproperties._size\n",
    "            child.set_fontweight('bold')\n",
    "            child.set_size(orig_fontsize*1.1)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    global_xlim = figure.axes[0].get_xlim()\n",
    "    return global_xlim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lung Type Classification Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "############# Lungsubtype\n",
    "meta_field        = 'luad'\n",
    "matching_field    = 'slides'\n",
    "resolution       = 2.0\n",
    "fold_number      = 4\n",
    "groupby          = 'leiden_%s' % resolution\n",
    "meta_folder      = 'lungsubtype_nn250'\n",
    "folds_pickle     = '/media/adalberto/Disk2/PhD_Workspace/utilities/files/LUADLUSC/lungsubtype_Institutions.pkl'\n",
    "type_composition = 'clr'\n",
    "min_tiles        = 100\n",
    "\n",
    "# Representations.\n",
    "h5_complete_path   = '%s/results/BarlowTwins_3/TCGAFFPE_LUADLUSC_5x_60pc_250K/h224_w224_n3_zdim128_filtered/hdf5_TCGAFFPE_LUADLUSC_5x_60pc_he_complete_lungsubtype_survival_filtered.h5' % main_path\n",
    "h5_additional_path = '%s/results/BarlowTwins_3/TCGAFFPE_LUADLUSC_5x_60pc_250K/h224_w224_n3_zdim128_filtered/NYUFFPE_LUADLUSC_5x_60pc/h224_w224_n3_zdim128/hdf5_NYUFFPE_LUADLUSC_5x_60pc_he_combined_filtered.h5' % main_path\n",
    "\n",
    "# File name and directories.\n",
    "file_name = h5_complete_path.split('/hdf5_')[1].split('.h5')[0] + '_%s__fold%s' % (groupby.replace('.', 'p'), fold_number)\n",
    "if h5_additional_path is not None: file_additional = h5_additional_path.split('/hdf5_')[1].split('.h5')[0] + '_%s__fold%s' % (groupby.replace('.', 'p'), fold_number)\n",
    "\n",
    "# Setup folder.\n",
    "main_cluster_path = h5_complete_path.split('hdf5_')[0]\n",
    "main_cluster_path = os.path.join(main_cluster_path, meta_folder)\n",
    "adatas_path       = os.path.join(main_cluster_path, 'adatas')\n",
    "figures_path      = os.path.join(main_cluster_path, 'figures')\n",
    "if not os.path.isdir(figures_path):\n",
    "\tos.makedirs(figures_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Read 5-fold cross-validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get folds from existing split.\n",
    "folds = load_existing_split(folds_pickle)\n",
    "\n",
    "# Path for alpha Logistic Regression results.\n",
    "main_cluster_path = h5_complete_path.split('hdf5_')[0]\n",
    "main_cluster_path = os.path.join(main_cluster_path, meta_folder)\n",
    "adatas_path       = os.path.join(main_cluster_path, 'adatas')\n",
    "\n",
    "data_res_folds = dict()\n",
    "data_res_folds[resolution] = dict()\n",
    "for i, fold in enumerate(folds):\n",
    "\t# Read CSV files for train, validation, test, and additional sets.\n",
    "\tdataframes, complete_df, leiden_clusters = read_csvs(adatas_path, matching_field, groupby, i, fold, h5_complete_path, h5_additional_path, additional_as_fold=False, force_fold=fold_number)\n",
    "\ttrain_df, valid_df, test_df, additional_df = dataframes\n",
    "\n",
    "\t# Check clusters and diversity within.\n",
    "\tframe_clusters, frame_samples = create_frames(train_df, groupby, meta_field, diversity_key=matching_field, reduction=2)\n",
    "\n",
    "\t# Create representations per sample: cluster % of total sample.\n",
    "\tdata, data_df, features = prepare_data_classes(dataframes, matching_field, meta_field, groupby, leiden_clusters, type_composition, min_tiles,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   use_conn=False, use_ratio=False, top_variance_feat=0)\n",
    "\n",
    "\t# Include features that are not the regular leiden clusters.\n",
    "\tframe_clusters = include_features_frame_clusters(frame_clusters, leiden_clusters, features, groupby)\n",
    "\n",
    "\t# Store representations.\n",
    "\tdata_res_folds[resolution][i] = {'data':data, 'data_df':data_df, 'complete_df':complete_df, 'features':features, 'frame_clusters':frame_clusters, 'leiden_clusters':leiden_clusters}\n",
    "\n",
    "\t# Information.\n",
    "\tprint('\\t\\tFold', i, 'Features:', len(features), 'Clusters:', len(leiden_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def model_log_odds(x):\n",
    "\tp = model.predict_log_proba(x)\n",
    "\treturn p[:,1] - p[:,0]\n",
    "\n",
    "# Cross-validation data.\n",
    "test_samples        = list()\n",
    "shap_values_cv      = list()\n",
    "shap_base_values_cv = list()\n",
    "shap_data_cv\t\t= list()\n",
    "coefficients         = list()\n",
    "for i, fold in enumerate(folds):\n",
    "\t\n",
    "\t# Load data for classification.\n",
    "\tdata_df         = data_res_folds[resolution][i]['data_df']\n",
    "\tfeatures        = data_res_folds[resolution][i]['features']\n",
    "\tleiden_clusters = data_res_folds[resolution][i]['leiden_clusters']\n",
    "\n",
    "\ttrain_df, valid_df, test_df, additional_df = data_df\n",
    "\n",
    "\t# a simple linear model\n",
    "\tmodel = sklearn.linear_model.LogisticRegression(penalty='l1', C=0.1, solver='liblinear', max_iter=100000)\n",
    "\tmodel.fit(train_df[leiden_clusters], train_df[meta_field])\n",
    "\tcoefficients.append(model.coef_)\n",
    "\n",
    "\tcomplete_df = pd.concat([test_df], axis=0)\n",
    "\tdata_plot = complete_df[leiden_clusters.tolist()]\n",
    "\tdata_plot.columns = data_plot.columns.astype(str)\n",
    "\n",
    "\texplainer = shap.Explainer(model_log_odds, train_df[leiden_clusters.tolist()], max_samples=train_df.shape[0])\n",
    "\tshap_values = explainer(data_plot)\n",
    "\tshap_values_cv.append(shap_values.values)\n",
    "\tshap_base_values_cv.append(shap_values.base_values)\n",
    "\tshap_data_cv.append(shap_values.data)\n",
    "\ttest_samples.append(data_plot)\n",
    "\n",
    "# Combine SHAP values for 5-fold test set.\n",
    "dataset_samples         = test_samples[0].copy(deep=True)\n",
    "shap_values_cv_all      = np.array(shap_values_cv[0])\n",
    "shap_base_values_cv_all = np.array(shap_base_values_cv[0])\n",
    "shap_data_cv_all        = np.array(shap_data_cv[0])\n",
    "for i in range(1, len(shap_values_cv)):\n",
    "\tdataset_samples = pd.concat([dataset_samples, test_samples[i]], axis=0)\n",
    "\tshap_values_cv_all      = np.concatenate([shap_values_cv_all, shap_values_cv[i]], axis=0)\n",
    "\tshap_base_values_cv_all = np.concatenate([shap_base_values_cv_all, shap_base_values_cv[i]], axis=0)\n",
    "\tshap_data_cv_all        = np.concatenate([shap_data_cv_all, shap_data_cv[i]], axis=0)\n",
    "\n",
    "# SHAP object.\n",
    "shap_cv             = copy.deepcopy(shap_values)\n",
    "shap_cv.values      = shap_values_cv_all\n",
    "shap_cv.base_values = shap_base_values_cv_all\n",
    "shap_cv.data        = shap_data_cv_all\n",
    "\n",
    "# Coefficients 5-fold cross-validation.\n",
    "coefficients = np.mean(np.array(coefficients), axis=0)\n",
    "coefficients = pd.DataFrame(coefficients.T, columns=['coef'])\n",
    "coefficients[groupby] = list(range(coefficients.shape[0]))\n",
    "coef_folds_pos = coefficients[coefficients['coef']>0]\n",
    "coef_folds_neg = coefficients[coefficients['coef']<0]\n",
    "hpc_pos = coef_folds_pos[groupby].values\n",
    "hpc_neg = coef_folds_neg[groupby].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Shap figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_features_from_shap(shap_cv, features_to_remove):\n",
    "\tshap_cv_removed = copy.deepcopy(shap_cv)\n",
    "\tfor feature in features_to_remove:\n",
    "\t\tshap_cv_removed.values[:,feature] = 0\n",
    "\t\tshap_cv_removed.data[:,feature] = 0\n",
    "\treturn shap_cv_removed\n",
    "\n",
    "for hpc_samples in [20, 10]:\n",
    "\t# Get order to display:\n",
    "\t# 1. Pro/Against event: Cox coefficient sign.\n",
    "\t# 2. Average absolute effect on the cohort: SHAP.\n",
    "\t# Select top X HPC impact for Pro/Against.\n",
    "\thpc_impact = list(reversed(np.argsort(shap_cv.abs.mean(0).values)))\n",
    "\tpos_order = list()\n",
    "\tneg_order = list()\n",
    "\tfor hpc in hpc_impact:\n",
    "\t\tif hpc in hpc_pos and len(pos_order+neg_order) < hpc_samples:\n",
    "\t\t\tpos_order.append(hpc)\n",
    "\t\telif hpc in hpc_neg and len(pos_order+neg_order) < hpc_samples:\n",
    "\t\t\tneg_order.append(hpc)\n",
    "\torder = pos_order + list(reversed(neg_order))\n",
    "\tdifference = list(set(features).difference(set(order)))\n",
    "\ttotal_order = order + list(difference)\n",
    "\n",
    "\t# Remove the features that are not going to be plotted for ascetics.\n",
    "\tshap_cv_removed = remove_features_from_shap(shap_cv=shap_cv, features_to_remove=difference)\n",
    "\n",
    "\t# Shap figure.\n",
    "\ttext1 = '\\nFavors LUSC'\n",
    "\ttext2 = 'Favors LUAD\\nSHAP Value\\nLog Odd Ratio'\n",
    "\tplot_shap_figure(shap_cv, order, total_order, text1, text2, plot_size=(25,20), fontsize_labels=24, fontsize_ticks=22, offset=135, cmap_bar=sns.diverging_palette(250, 20, as_cmap=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Cox Survival Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Common variables.\n",
    "type_composition = 'clr'\n",
    "max_months       = 15.0*15.0\n",
    "matching_field    = 'samples'\n",
    "diversity_key    = None\n",
    "\n",
    "# Representation files.\n",
    "h5_complete_path   = '%s/results/BarlowTwins_3/TCGAFFPE_LUADLUSC_5x_60pc_250K/h224_w224_n3_zdim128_filtered/hdf5_TCGAFFPE_LUADLUSC_5x_60pc_he_complete_lungsubtype_survival_filtered.h5' % main_path\n",
    "h5_additional_path = '%s/results/BarlowTwins_3/TCGAFFPE_LUADLUSC_5x_60pc_250K/h224_w224_n3_zdim128_filtered/NYU300LUAD_Survival_5x_60pc/h224_w224_n3_zdim128/hdf5_NYU300LUAD_Survival_5x_60pc_he_train_overall_progression_free_surival_filtered.h5' % main_path\n",
    "\n",
    "# Cluster configuration details.\n",
    "resolution = 2.0\n",
    "force_fold  = 0\n",
    "\n",
    "# PFS related variables.\n",
    "event_ind_field  = 'pfs_event_ind'\n",
    "event_data_field = 'pfs_event_data'\n",
    "additional_as_fold = True\n",
    "meta_folder        = 'luad_progression_free_survival_nn250_fold0_NYU_v3_csNP'\n",
    "folds_pickle       = '%s/utilities/files/LUAD/progression_free_survival_NYU_folds_v3_csNP.pkl' % main_path\n",
    "\n",
    "# OS related variables.\n",
    "# event_ind_field     = 'os_event_ind'\n",
    "# event_data_field    = 'os_event_data'\n",
    "# additional_as_fold = False\n",
    "# meta_folder        = 'luad_overall_survival_nn250_fold0_NYU_v3'\n",
    "# folds_pickle       = '%s/utilities/files/LUAD/overall_survival_TCGA_folds.pkl'  % main_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Read 5-fold cross-validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Setup folder.\n",
    "main_cluster_path = h5_complete_path.split('hdf5_')[0]\n",
    "main_cluster_path = os.path.join(main_cluster_path, meta_folder)\n",
    "adatas_path       = os.path.join(main_cluster_path, 'adatas')\n",
    "\n",
    "# Get folds from existing split.\n",
    "folds = load_existing_split(folds_pickle)\n",
    "\n",
    "# If diversity key is not specified, use the key that represents samples.\n",
    "if diversity_key is None:\n",
    "\tdiversity_key = matching_field\n",
    "\n",
    "# Loading data first.\n",
    "print('Loading data:')\n",
    "data_res_folds = dict()\n",
    "groupby = 'leiden_%s' % resolution\n",
    "print('\\tResolution', groupby)\n",
    "data_res_folds[resolution] = dict()\n",
    "for i, fold in enumerate(folds):\n",
    "\t# Read CSV files for train, validation, test, and additional sets.\n",
    "\tdataframes, complete_df, leiden_clusters = read_csvs(adatas_path, matching_field, groupby, i, fold, h5_complete_path, h5_additional_path, additional_as_fold, force_fold)\n",
    "\n",
    "\t# Check clusters and diversity within.\n",
    "\tframe_clusters, frame_samples = create_frames(complete_df, groupby, event_ind_field, diversity_key=matching_field, reduction=2)\n",
    "\n",
    "\t# Prepare data for COX.\n",
    "\tdata, datas_all, features = prepare_data_survival(dataframes, groupby, leiden_clusters, type_composition, max_months, matching_field, event_ind_field, event_data_field, 100, use_conn=False, use_ratio=False, top_variance_feat=0, remove_clusters=None)\n",
    "\tif i==0:\n",
    "\t\tdata_percent, datas_all_percent, _ = prepare_data_survival(dataframes, groupby, leiden_clusters, 'percent', max_months, matching_field, event_ind_field, event_data_field, 100, use_conn=False, use_ratio=False, top_variance_feat=0, remove_clusters=None)\n",
    "\n",
    "\t# Store representations.\n",
    "\tdata_res_folds[resolution][i] = {'data':data, 'data_all':datas_all, 'features':features, 'frame_clusters': frame_clusters}\n",
    "\n",
    "\t# Information\n",
    "\tprint('\\t\\tFold', i, 'Features:', len(features), 'Clusters:', len(leiden_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "l1_ratio = 0.0\n",
    "if 'os_' in event_data_field:\n",
    "\talpha    = 1.\n",
    "else:\n",
    "\talpha    = 0.27\n",
    "\n",
    "# Cross-validation data.\n",
    "test_samples        = list()\n",
    "shap_values_cv      = list()\n",
    "shap_base_values_cv = list()\n",
    "shap_data_cv\t\t= list()\n",
    "\n",
    "data_all = pd.concat([datas_all[0][0], datas_all[2][0]], axis=0)\n",
    "\n",
    "groupby = 'leiden_%s' % resolution\n",
    "folds_cis = list()\n",
    "models    = list()\n",
    "cox_folds = list()\n",
    "\n",
    "risk_groups     = [pd.DataFrame(), pd.DataFrame()]\n",
    "additional_risk = pd.DataFrame()\n",
    "num_folds = len(data_res_folds[resolution].keys())\n",
    "for fold_cox in range(num_folds):\n",
    "\t# Load data.\n",
    "\tdatas     = data_res_folds[resolution][fold_cox]['data']\n",
    "\tdatas_all = data_res_folds[resolution][fold_cox]['data_all']\n",
    "\tfeatures  = data_res_folds[resolution][fold_cox]['features']\n",
    "\tframe_clusters = data_res_folds[resolution][fold_cox]['frame_clusters']\n",
    "\ttrain, set_name   = datas[0]\n",
    "\ttest, set_name    = datas[2]\n",
    "\ttest_df, set_name = datas_all[2]\n",
    "\n",
    "\t# Train Cox Proportional Hazard.\n",
    "\tcph = CoxPHFitter(penalizer=alpha, l1_ratio=l1_ratio)\n",
    "\tcph.fit(train[features+[event_data_field, event_ind_field]], duration_col=event_data_field, event_col=event_ind_field, show_progress=False, robust=True)\n",
    "\tmodels.append(cph)\n",
    "\n",
    "\t# Partial hazard prediction for each list.\n",
    "\tpredictions = list()\n",
    "\tfor data, set_name in datas:\n",
    "\t\tif data is not None:\n",
    "\t\t\tpred = cph.predict_partial_hazard(data[features+[event_data_field, event_ind_field]])\n",
    "\t\telse:\n",
    "\t\t\tpred = None\n",
    "\t\tpredictions.append((pred, set_name))\n",
    "\n",
    "\t# Keep track of Cox coefficients to find the average of 5-fold cross-validation.\n",
    "\tsummary_table = cph.summary\n",
    "\tif frame_clusters is not None:\n",
    "\t\tframe_clusters = frame_clusters.sort_values(by=groupby)\n",
    "\t\tfor column in ['coef', 'coef lower 95%', 'coef upper 95%', 'p']:\n",
    "\t\t\tfor cluster_id in features:\n",
    "\t\t\t\tframe_clusters.loc[frame_clusters[groupby]==cluster_id, column] = summary_table.loc[cluster_id, column].astype(np.float32)\n",
    "\t\tframe_clusters = frame_clusters.sort_values(by='coef')\n",
    "\t\tframe_clusters = frame_clusters[frame_clusters[groupby].isin(features)]\n",
    "\t\tcox_folds.append(frame_clusters)\n",
    "\n",
    "\thigh_lows = get_high_low_risks(predictions, datas_all, fold_cox, matching_field, q_buckets=2)\n",
    "\trisk_groups, additional_risk = combine_risk_groups(risk_groups, additional_risk, high_lows, fold_cox, num_folds, matching_field, event_ind_field, event_data_field)\n",
    "\n",
    "\texplainer = shap.Explainer(cph.predict_log_partial_hazard, train[features], max_samples=train.shape[0])\n",
    "\tshap_values = explainer(test_df[features])\n",
    "\tshap_values_cv.append(shap_values.values)\n",
    "\tshap_base_values_cv.append(shap_values.base_values)\n",
    "\tshap_data_cv.append(shap_values.data)\n",
    "\ttest_samples.append(test_df)\n",
    "\n",
    "\tcis = evalutaion_survival(datas, predictions, event_ind_field=event_ind_field, event_data_field=event_data_field)\n",
    "\tfolds_cis.append(cis)\n",
    "\n",
    "# Keep track of Cox coef (not CI).\n",
    "cox_folds = pd.concat(cox_folds, axis=0)\n",
    "cox_folds = cox_folds.groupby(groupby).mean().reset_index()\n",
    "cox_folds = cox_folds.sort_values(by='coef')\n",
    "cox_folds = cox_folds[[groupby, 'coef']]\n",
    "cox_folds_pos = cox_folds[cox_folds['coef'] > 0].sort_values(by='coef', ascending=False)\n",
    "cox_folds_neg = cox_folds[cox_folds['coef'] < 0].sort_values(by='coef', ascending=True)\n",
    "hpc_pos = cox_folds_pos[groupby].values\n",
    "hpc_neg = cox_folds_neg[groupby].values\n",
    "\n",
    "# Combine SHAP values for 5-fold test set.\n",
    "dataset_samples         = test_samples[0].copy(deep=True)\n",
    "shap_values_cv_all      = np.array(shap_values_cv[0])\n",
    "shap_base_values_cv_all = np.array(shap_base_values_cv[0])\n",
    "shap_data_cv_all        = np.array(shap_data_cv[0])\n",
    "for i in range(1, len(shap_values_cv)):\n",
    "\tdataset_samples = pd.concat([dataset_samples, test_samples[i]], axis=0)\n",
    "\tshap_values_cv_all      = np.concatenate([shap_values_cv_all, shap_values_cv[i]], axis=0)\n",
    "\tshap_base_values_cv_all = np.concatenate([shap_base_values_cv_all, shap_base_values_cv[i]], axis=0)\n",
    "\tshap_data_cv_all        = np.concatenate([shap_data_cv_all, shap_data_cv[i]], axis=0)\n",
    "dataset_samples = dataset_samples.reset_index()\n",
    "dataset_samples.columns = dataset_samples.columns.astype(str)\n",
    "\n",
    "# SHAP object.\n",
    "shap_cv             = copy.deepcopy(shap_values)\n",
    "shap_cv.values      = shap_values_cv_all\n",
    "shap_cv.base_values = shap_base_values_cv_all\n",
    "shap_cv.data        = shap_data_cv_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## C-Index Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_data = list()\n",
    "for i, fold_data in enumerate(folds_cis):\n",
    "\tfor c_i, set_ in fold_data:\n",
    "\t\tif c_i is None: continue\n",
    "\t\tentry = (resolution, i, c_i, set_)\n",
    "\t\tall_data.append(entry)\n",
    "columns=['Leiden','Fold','C-Index','Set']\n",
    "all_data = pd.DataFrame(all_data, columns=columns)\n",
    "\n",
    "plot_performance_cindex(all_data, title='Leiden + Cox Proportional Hazards')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Shap figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_features_from_shap(shap_cv, features_to_remove):\n",
    "\tshap_cv_removed = copy.deepcopy(shap_cv)\n",
    "\tfor feature in features_to_remove:\n",
    "\t\tshap_cv_removed.values[:,feature] = 0\n",
    "\t\tshap_cv_removed.data[:,feature] = 0\n",
    "\treturn shap_cv_removed\n",
    "\n",
    "for hpc_samples in [20, 10]:\n",
    "\t# Get order to display:\n",
    "\t# 1. Pro/Against event: Cox coefficient sign.\n",
    "\t# 2. Average absolute effect on the cohort: SHAP.\n",
    "\t# Select top X HPC impact for Pro/Against.\n",
    "\thpc_impact = list(reversed(np.argsort(shap_cv.abs.mean(0).values)))\n",
    "\tpos_order = list()\n",
    "\tneg_order = list()\n",
    "\tfor hpc in hpc_impact:\n",
    "\t\tif hpc in hpc_pos and len(pos_order+neg_order) < hpc_samples:\n",
    "\t\t\tpos_order.append(hpc)\n",
    "\t\telif hpc in hpc_neg and len(pos_order+neg_order) < hpc_samples:\n",
    "\t\t\tneg_order.append(hpc)\n",
    "\torder = pos_order + list(reversed(neg_order))\n",
    "\tdifference = list(set(features).difference(set(order)))\n",
    "\ttotal_order = order + list(difference)\n",
    "\n",
    "\t# Remove the features that are not going to be plotted for ascetics.\n",
    "\tshap_cv_removed = remove_features_from_shap(shap_cv=shap_cv, features_to_remove=difference)\n",
    "\n",
    "\t# Shap figure.\n",
    "\tif 'os_' in event_ind_field:\n",
    "\t\ttext1 = '\\nFavors Survival'\n",
    "\t\ttext2 = 'Favors Death\\nSHAP Value\\nLog Hazard Ratio'\n",
    "\telse:\n",
    "\t\ttext1 = '\\nAgainst Recurrence'\n",
    "\t\ttext2 = 'Favors Recurrence\\nSHAP Value\\nLog Hazard Ratio'\n",
    "\n",
    "\tfontsize_labels = 34\n",
    "\tfontsize_ticks  = 34\n",
    "\tplot_shap_figure(shap_cv_removed, order, total_order, text1, text2, plot_size=(25,20), fontsize_labels=fontsize_labels, fontsize_ticks=fontsize_ticks, \n",
    "\t\t\t\t\t offset=100, cmap_bar=sns.diverging_palette(250, 20, as_cmap=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_perc = pd.concat([datas_all_percent[0][0], datas_all_percent[2][0]])\n",
    "data_df_perc.columns = data_df_perc.columns.astype(str)\n",
    "if data_df_perc.shape[0]!=dataset_samples.shape[0]:\n",
    "    print('Error number of samples does not match.')\n",
    "\n",
    "data_df_perc = data_df_perc.set_index('samples')\n",
    "data_df_perc = data_df_perc.reindex(dataset_samples.samples.tolist())\n",
    "data_df_perc = data_df_perc.reset_index()\n",
    "\n",
    "num_features    = 'all'\n",
    "# num_features    = 15\n",
    "num_samples     = 10\n",
    "fontsize_labels = 20\n",
    "fontsize_ticks  = 18\n",
    "fontsize_legend = 16\n",
    "l_box_w         = 3\n",
    "offset=75\n",
    "show_legend = True\n",
    "\n",
    "# Shap figure.\n",
    "if 'os_' in event_ind_field:\n",
    "    text1 = '\\nFavors Survival'\n",
    "    text2 = 'Favors Death\\nSHAP Value\\nLog Hazard Ratio'\n",
    "else:\n",
    "    text1 = '\\nAgainst Recurrence'\n",
    "    text2 = 'Favors Recurrence\\nSHAP Value\\nLog Hazard Ratio'\n",
    "text = text1 + ''.join([' ']*(offset-len(text1+text2))) + text2\n",
    "\n",
    "low_risk = risk_groups[0]\n",
    "sample_low_risk = low_risk.sort_values(by=['hazard']).iloc[:num_samples]['samples']\n",
    "low_risk_ind = dataset_samples[dataset_samples[matching_field].isin(sample_low_risk)].index\n",
    "data_df_perc_sample = data_df_perc[data_df_perc[matching_field].isin(sample_low_risk)]\n",
    "\n",
    "high_risk = risk_groups[1]\n",
    "sample_high_risk = high_risk.sort_values(by=['hazard'], ascending=False).iloc[:num_samples]['samples']\n",
    "high_risk_ind = dataset_samples[dataset_samples[matching_field].isin(sample_high_risk)].index\n",
    "data_df_perc_sample = data_df_perc[data_df_perc[matching_field].isin(sample_high_risk)]\n",
    "\n",
    "shap_base_values         = shap_cv.base_values\n",
    "shap_values              = shap_cv.values\n",
    "dataset_samples_features = dataset_samples[leiden_clusters.astype(str)]\n",
    "\n",
    "xlim = plot_shap_decision(shap_base_values, shap_values, dataset_samples_features, low_risk_ind, high_risk_ind, num_features, show_legend, text, fontsize_labels, fontsize_ticks, fontsize_legend, l_box_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_legend = True\n",
    "num_samples = 47\n",
    "\n",
    "# ascending = False\n",
    "# match = 0\n",
    "# name_plot = 'censored_long_followup'\n",
    "\n",
    "ascending = True\n",
    "match = 1\n",
    "name_plot = 'event_short_time'\n",
    "\n",
    "dataset_samples_copy = dataset_samples.sort_values(by=event_data_field, ascending=ascending).copy(deep=True)\n",
    "dataset_samples_copy = dataset_samples_copy[dataset_samples_copy[event_ind_field]==match]\n",
    "samples_to_get       = dataset_samples_copy.iloc[:num_samples]['samples']\n",
    "\n",
    "low_risk = risk_groups[0]\n",
    "low_risk        = low_risk[low_risk[matching_field].isin(samples_to_get)]\n",
    "sample_low_risk = low_risk.sort_values(by=['hazard']).iloc[:num_samples]['samples']\n",
    "low_risk_ind = dataset_samples[dataset_samples[matching_field].isin(sample_low_risk)].index\n",
    "data_df_perc_sample = data_df_perc[data_df_perc[matching_field].isin(sample_low_risk)]\n",
    "\n",
    "high_risk = risk_groups[1]\n",
    "high_risk        = high_risk[high_risk[matching_field].isin(samples_to_get)]\n",
    "sample_high_risk = high_risk.sort_values(by=['hazard'], ascending=False).iloc[:num_samples]['samples']\n",
    "high_risk_ind = dataset_samples[dataset_samples[matching_field].isin(sample_high_risk)].index\n",
    "data_df_perc_sample = data_df_perc[data_df_perc[matching_field].isin(sample_high_risk)]\n",
    "\n",
    "shap_base_values         = shap_cv.base_values\n",
    "shap_values              = shap_cv.values\n",
    "dataset_samples_features = dataset_samples[leiden_clusters.astype(str)]\n",
    "\n",
    "plot_shap_decision(shap_base_values, shap_values, dataset_samples_features, low_risk_ind, high_risk_ind, num_features, show_legend, text, fontsize_labels, fontsize_ticks, fontsize_legend, l_box_w, xlim)\n",
    "dataset_samples_copy[dataset_samples_copy[matching_field].isin(samples_to_get)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.DataFrame()\n",
    "for hpc in leiden_clusters.astype(str):\n",
    "    hpc_list = list()\n",
    "    for line_a, line_b in zip(dataset_samples[hpc].values, data_df_perc[hpc].values):\n",
    "        line = str(np.round(line_a,3)) + ' / ' + str(np.round(line_b*100,2))\n",
    "        hpc_list.append(line)\n",
    "    combined_df[hpc] = hpc_list\n",
    "combined_df.columns = leiden_clusters.astype(str)\n",
    "combined_df = combined_df.set_index(dataset_samples.index)\n",
    "combined_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shap_decision_ind(shap_base_values, shap_values, dataset_samples_features, leiden_clusters, low_risk_ind, high_risk_ind, num_features, show_legend, text, fontsize_labels, fontsize_ticks, fontsize_legend, l_box_w, xlim=None, annotation=False, y_perc=False):\n",
    "\n",
    "    if low_risk_ind is None:\n",
    "        risk_ind = high_risk_ind\n",
    "    elif high_risk_ind is None:\n",
    "        risk_ind = low_risk_ind\n",
    "    else:\n",
    "        risk_ind = np.concatenate([low_risk_ind.values, high_risk_ind.values])\n",
    "\n",
    "    # Legend handling.\n",
    "    if show_legend:\n",
    "        legend_labels = ['Low Risk' for s in range(low_risk_ind.values.shape[0])]\n",
    "        legend_labels.extend(['High Risk' for s in range(high_risk_ind.values.shape[0])])\n",
    "    else:\n",
    "        legend_labels = None\n",
    "    # Number of HPCs to display\n",
    "    if num_features == 'all':\n",
    "        feature_display_range = slice(None, None, -1)\n",
    "    else:\n",
    "        feature_display_range = slice(-1, -num_features, -1)\n",
    "\n",
    "    shap_base_plot = [shap_base_values[risk_ind[0]]]\n",
    "    shap_cv_values = np.array(shap_cv.values[risk_ind[0]].reshape((1,-1)))\n",
    "    data_values    = np.array(dataset_samples_features.loc[risk_ind[0]].values.reshape((1,-1)))\n",
    "\n",
    "    for ind in risk_ind[1:]:        \n",
    "        shap_base_plot.append(shap_base_values[ind])\n",
    "        shap_cv_values = np.concatenate([shap_cv_values, shap_cv.values[ind].reshape((1,-1))])\n",
    "        data_values    = np.concatenate([data_values, dataset_samples_features.loc[ind].values.reshape((1,-1))])\n",
    "\n",
    "    # SHAP decision plot.\n",
    "    if not annotation:\n",
    "        shap.decision_plot(base_value=np.mean(shap_base_plot), shap_values=shap_cv_values, feature_names=leiden_clusters.astype(str), xlim=xlim,\n",
    "                        feature_order='importance', feature_display_range=feature_display_range, show=False, legend_labels=legend_labels, legend_location='lower right')\n",
    "    else:\n",
    "        shap.decision_plot(base_value=np.mean(shap_base_plot), shap_values=shap_cv_values, features=dataset_samples_features.loc[risk_ind], xlim=xlim,\n",
    "                        feature_order='importance', feature_display_range=feature_display_range, show=False, legend_labels=legend_labels, legend_location='lower right')\n",
    "    # shap.decision_plot(base_value=np.mean(shap_base_plot), shap_values=shap_cv_values, xlim=xlim,\n",
    "                    # feature_order='importance', feature_display_range=feature_display_range, show=False, legend_labels=legend_labels, legend_location='lower right')\n",
    "\n",
    "    # Ascetic fixes.\n",
    "    figure = plt.gcf()\n",
    "    figure.axes[0].set_xlabel(text, fontweight='bold', fontsize=fontsize_labels)\n",
    "    if y_perc:\n",
    "        figure.axes[0].set_ylabel('HPC Percentage', fontweight='bold', fontsize=fontsize_labels)\n",
    "    else:\n",
    "        figure.axes[0].set_ylabel('Histomorpholical Phenotype Cluster (HPC)', fontweight='bold', fontsize=fontsize_labels)\n",
    "\n",
    "    if y_perc:\n",
    "        yticklabels = list()\n",
    "        for label_tick in figure.axes[0].get_yticklabels():\n",
    "            hpc = label_tick._text\n",
    "            anno = dataset_samples_features.loc[risk_ind, hpc].values[0]\n",
    "            yticklabels.append(str(anno).split('/')[1]+'%')\n",
    "        # plot.set_xticks([mini, self.center, maxi], fontsize=fontsize)\n",
    "        figure.axes[0].set_yticklabels(yticklabels)\n",
    "    for ax in [figure.axes[0]]:\n",
    "        for tick in ax.xaxis.get_major_ticks():\n",
    "            tick.label1.set_fontweight('bold')\n",
    "            tick.label2.set_fontweight('bold')\n",
    "            tick.label1.set_fontsize(fontsize_ticks)\n",
    "            tick.label2.set_fontsize(fontsize_ticks)            \n",
    "        for tick in ax.yaxis.get_major_ticks():\n",
    "            tick.label1.set_fontweight('bold')\n",
    "            tick.label2.set_fontweight('bold')\n",
    "            tick.label1.set_fontsize(fontsize_ticks)\n",
    "            tick.label2.set_fontsize(fontsize_ticks)\n",
    "            # if fontscale_anno == 0.0:\n",
    "            # print(tick.label1)\n",
    "            # print(tick.label2)\n",
    "        for axis in ['bottom','left','right']:\n",
    "            ax.spines[axis].set_linewidth(4)\n",
    "    \n",
    "    # Legend for High/Low risk group.\n",
    "    if show_legend:\n",
    "        legend = figure.axes[0].legend_\n",
    "        new_handles = [legend.legendHandles[0], legend.legendHandles[-1]]\n",
    "        new_texts   = [legend.get_texts()[0]._text, legend.get_texts()[-1]._text]\n",
    "        map_dict = dict()\n",
    "        for handle, text in zip(legend.legendHandles, legend.get_texts()):\n",
    "            map_dict[handle._label] = text._text\n",
    "\n",
    "        legend.remove()\n",
    "        legend = figure.axes[0].legend(new_handles, new_texts, loc='lower right')\n",
    "        legend.get_frame().set_linewidth(l_box_w)\n",
    "        for i, handler in enumerate(legend.legendHandles):\n",
    "            handler.set_linewidth(5)\n",
    "            handler.set_color('Blue')\n",
    "            if i!=0:\n",
    "                handler.set_color('Red')\n",
    "\n",
    "        for text in legend.get_texts():\n",
    "            text.set_size(fontsize_legend)\n",
    "            text.set_fontweight('bold')\n",
    "\n",
    "        # Re-color lines for High/Low risk group.\n",
    "        continuous_lines = [child for child in figure.axes[0]._children if child._linestyle == '-' and len(child._x) > 2]\n",
    "        i = 0 \n",
    "        for child in continuous_lines:\n",
    "            risk_name = map_dict[child._label]\n",
    "            if risk_name == 'Low Risk':\n",
    "                child.set_color('Blue')\n",
    "            elif risk_name == 'High Risk':\n",
    "                child.set_color('Red')\n",
    "            i+=1\n",
    "    else:\n",
    "        annotations_val  = [child for child in figure.axes[0]._children if not isinstance(child, matplotlib.lines.Line2D)]\n",
    "        for child in annotations_val:\n",
    "            orig_fontsize = child._fontproperties._size\n",
    "            child.set_fontweight('bold')\n",
    "            child.set_size(orig_fontsize*1.1)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    global_xlim = figure.axes[0].get_xlim()\n",
    "    return global_xlim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_risk = risk_groups[1]\n",
    "high_risk = high_risk[high_risk.h_bin==1]\n",
    "sample_high_risk = high_risk.sort_values(by=['hazard'], ascending=False).iloc[[1]]['samples']\n",
    "high_risk_ind = dataset_samples[dataset_samples[matching_field].isin(sample_high_risk)].index\n",
    "dataset_samples[dataset_samples[matching_field].isin(sample_high_risk)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_legend = False\n",
    "\n",
    "low_risk = risk_groups[0]\n",
    "low_risk = low_risk[low_risk.h_bin==0]\n",
    "sample_low_risk = low_risk.sort_values(by=['hazard']).iloc[0]['samples']\n",
    "low_risk_ind = dataset_samples[dataset_samples[matching_field].isin([sample_low_risk])].index\n",
    "\n",
    "high_risk = risk_groups[1]\n",
    "high_risk = high_risk[high_risk.h_bin==1]\n",
    "sample_high_risk = high_risk.sort_values(by=['hazard'], ascending=False).iloc[[1]]['samples']\n",
    "high_risk_ind = dataset_samples[dataset_samples[matching_field].isin(sample_high_risk)].index\n",
    "\n",
    "for l_ind, h_ind in [(low_risk_ind, None), (None, high_risk_ind)]:\n",
    "    # for values_to_use in [combined_df, data_df_perc, dataset_samples]:\n",
    "    for values_to_use in [combined_df]:\n",
    "\n",
    "        shap_base_values         = shap_cv.base_values\n",
    "        shap_values              = shap_cv.values\n",
    "        dataset_samples_features = values_to_use[leiden_clusters.astype(str)]\n",
    "\n",
    "        plot_shap_decision_ind(shap_base_values, shap_values, dataset_samples_features, leiden_clusters, l_ind, h_ind, num_features, show_legend, text, \n",
    "                               fontsize_labels, fontsize_ticks, fontsize_legend, l_box_w, xlim, annotation=False, y_perc=False)\n",
    "        plot_shap_decision_ind(shap_base_values, shap_values, dataset_samples_features, leiden_clusters, l_ind, h_ind, num_features, show_legend, text, \n",
    "                               fontsize_labels, fontsize_ticks, fontsize_legend, l_box_w, xlim, annotation=False, y_perc=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skbio.stats.composition import clr, ilr, alr, multiplicative_replacement\n",
    "\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "for ind in [0,1,2,3]:\n",
    "    \n",
    "    sample_low_risk = low_risk.sort_values(by=['hazard']).iloc[[ind]]['samples']\n",
    "    sample_high_risk = high_risk.sort_values(by=['hazard'], ascending=False).iloc[[ind]]['samples']\n",
    "\n",
    "    original_rep_h = data_df_perc[data_df_perc[matching_field].isin(sample_high_risk)][leiden_clusters.astype(str)].values[0,:].tolist()\n",
    "    multrepl_rep_h = multiplicative_replacement(np.reshape(original_rep_h, (1,-1)))\n",
    "    clr_rep_h      = clr(np.reshape(multrepl_rep_h, (1,-1)))\n",
    "\n",
    "    delta_h = np.round(1/(multrepl_rep_h.shape[0]*multrepl_rep_h.shape[0]), 4)\n",
    "    g_x_h   = np.round(gmean(multrepl_rep_h), 4)\n",
    "\n",
    "    original_rep_l = data_df_perc[data_df_perc[matching_field].isin(sample_low_risk)][leiden_clusters.astype(str)].values[0,:].tolist()\n",
    "    multrepl_rep_l = multiplicative_replacement(np.reshape(original_rep_l, (1,-1)))\n",
    "    clr_rep_l      = clr(np.reshape(multrepl_rep_l, (1,-1)))\n",
    "\n",
    "    delta_l = np.round(1/(multrepl_rep_l.shape[0]*multrepl_rep_l.shape[0]),4)\n",
    "    g_x_l   = np.round(gmean(multrepl_rep_l), 4)\n",
    "\n",
    "\n",
    "    fig   = plt.figure(figsize=(30,17))\n",
    "\n",
    "    ax    = fig.add_subplot(3, 2, 1)\n",
    "    ax.set_title('%s - High Risk\\nCompositional Data Original' % sample_high_risk.values[0])\n",
    "    ax.bar(x=leiden_clusters.tolist(), height=original_rep_h, tick_label=leiden_clusters)\n",
    "    ax.set_ylim([0.0,0.9])\n",
    "    ax    = fig.add_subplot(3, 2, 2)\n",
    "    ax.set_title('%s - Low Risk\\nCompositional Data Original' % sample_low_risk.values[0])\n",
    "    ax.bar(x=leiden_clusters.tolist(), height=original_rep_l, tick_label=leiden_clusters)\n",
    "    ax.set_ylim([0.0,0.9])\n",
    "\n",
    "    ax    = fig.add_subplot(3, 2, 3)\n",
    "    ax.set_title('Compositional Data Multiplicative Replacement\\ndelta=1/N^2=%s' % delta_h)\n",
    "    ax.bar(x=leiden_clusters.tolist(), height=multrepl_rep_h, tick_label=leiden_clusters)\n",
    "    ax.set_ylim([0.0,0.9])\n",
    "    ax    = fig.add_subplot(3, 2, 4)\n",
    "    ax.set_title('Compositional Data Multiplicative Replacement\\ndelta=1/N^2=%s' % delta_l)\n",
    "    ax.bar(x=leiden_clusters.tolist(), height=multrepl_rep_l, tick_label=leiden_clusters)\n",
    "    ax.set_ylim([0.0,0.9])\n",
    "\n",
    "    ax    = fig.add_subplot(3, 2, 5)\n",
    "    ax.set_title('Euclidean Data Centered Log Ratio\\ng(x)=%s' % g_x_h)\n",
    "    ax.bar(x=leiden_clusters.tolist(), height=clr_rep_h, tick_label=leiden_clusters)\n",
    "    ax.set_ylim([-2.5,7])\n",
    "    ax    = fig.add_subplot(3, 2, 6)\n",
    "    ax.set_title('Euclidean Data Centered Log Ratio\\ng(x)=%s' % g_x_l)\n",
    "    ax.bar(x=leiden_clusters.tolist(), height=clr_rep_l, tick_label=leiden_clusters)\n",
    "    ax.set_ylim([-2.5,7])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
